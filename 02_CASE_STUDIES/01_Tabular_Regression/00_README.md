# Pilot A — Tabular Regression (TreeSHAP + DiCE · Technical + Social Trust)

### Context  
This case study belongs to the *CASE_STUDIES* collection and constitutes the **first pilot** of the *XAI-TrustFramework* empirical validation.  
It combines **TreeSHAP** (attributive reasoning) and **DiCE** (counterfactual reasoning) to assess whether the *theoretical trust properties* of both techniques hold in practice when applied to a **clinical regression model**.

---

## Research Focus

> **To what extent do the explanations generated by TreeSHAP and DiCE preserve their theoretical guarantees of fidelity, completeness, stability, and actionability when applied to a clinical regression model, and how can these properties jointly inform the technical and social dimensions of trust?**

---

## Objective

This pilot uses the open clinical dataset `sklearn.diabetes` to demonstrate that:
- The **technical guarantees** of TreeSHAP (*local accuracy*, *consistency*, *missingness*)  
  can be empirically validated through *fidelity*, *completeness*, and *stability* metrics.  
- The **human-centered guarantees** of DiCE (*proximity*, *feasibility*, *diversity*)  
  can be operationalized through *actionability* and *plausibility* metrics.  

Together, they provide a combined view of **trustworthiness** that connects *model reliability* (technical trust) with *human interpretability and empowerment* (social trust).

---

## Experimental Setup

| Component | Description |
|------------|-------------|
| **Data Type** | Tabular (clinical data) |
| **Task Type** | Regression |
| **Base Model** | `RandomForestRegressor` |
| **XAI Techniques** | `TreeSHAP` (attributive) + `DiCE` (counterfactual) |
| **Trust Notions** | Fidelity, Completeness, Stability, Actionability |
| **Trust Dimensions** | Technical + Social |

---

## Workflow

01_Tabular_Regression_TreeSHAP_DiCE_Technical_SocialTrust/
├── data/
│ ├── raw/ ← diabetes.csv (public dataset)
│ └── processed/ ← train/val/test splits
├── notebooks/
│ ├── 01_BASEMODEL_RF.ipynb ← train baseline RandomForest model
│ ├── 02_SHAP_Explainer.ipynb ← compute TreeSHAP explanations
│ ├── 03_DiCE_Explainer.ipynb ← generate counterfactuals
│ └── 04_METRICS_EVALUATION.ipynb ← compute trust metrics (SHAP + DiCE)
├── results/
│ ├── metrics_summary.csv
│ ├── stability_analysis.csv
│ ├── dice_counterfactuals.csv
│ └── visuals/
└── configs/
├── priors_clinical.yaml
├── dice_constraints.yaml
└── seeds.json


---

## Metrics Operationalized

| Metric | Definition | Technique | Theoretical Property | Trust Dimension |
|---------|-------------|------------|----------------------|-----------------|
| **Fidelity** | Agreement between model predictions and additive reconstruction. | TreeSHAP | Local accuracy | Technical |
| **Completeness** | Proportion of instances where Σφᵢ(x) + φ₀ ≈ f(x). | TreeSHAP | Additivity / Missingness | Technical |
| **Stability** | Robustness of SHAP vectors under small input perturbations. | TreeSHAP | Consistency | Technical |
| **Actionability** | Ability to propose realistic, feasible counterfactuals to achieve target outcomes. | DiCE | Proximity / Feasibility | Social |
| **Diversity** | Extent to which generated counterfactuals provide distinct alternatives. | DiCE | Diversity | Social |
| **Plausibility** | Logical or domain validity of counterfactuals under given constraints. | DiCE | Feasibility | Social |

---

## Implementation Notes

- **TreeSHAP**  
  - Explainer: `shap.TreeExplainer(model, feature_perturbation='interventional')`  
  - Metrics implemented in `04_METRICS_EVALUATION.ipynb` using validation data.  
  - Stability tested via ε-perturbations and correlation/cosine similarity.

- **DiCE**  
  - Library: [`dice-ml`](https://github.com/interpretml/DiCE).  
  - Config file: `configs/dice_constraints.yaml` defines bounds, immutable variables, and costs.  
  - Counterfactuals generated for selected validation instances with target outcome ranges.  
  - Metrics: proximity (L1 distance), feasibility ratio, diversity index.

---

## Expected Outputs

| File | Description |
|------|--------------|
| `results/metrics_summary.csv` | Fidelity, completeness, stability, actionability metrics. |
| `results/stability_analysis.csv` | Instance-level correlation and cosine metrics. |
| `results/dice_counterfactuals.csv` | Counterfactual examples and their feasibility scores. |
| `results/visuals/` | SHAP plots (beeswarm, importance) and DiCE visualization tables. |

---

## Interpretation of Results

| Trust Notion | Meaning | Primary Stakeholder |
|---------------|----------|---------------------|
| **Fidelity** | How faithfully explanations reconstruct the model’s reasoning. | Engineer / Auditor |
| **Completeness** | Whether all relevant contributions are captured. | Regulator |
| **Stability** | Whether explanations remain consistent under small changes. | Developer |
| **Actionability** | Whether users can act meaningfully on model outputs. | Patient / Decision-maker |
| **Diversity** | Whether explanations offer more than one viable option. | End-user |
| **Plausibility** | Whether counterfactuals are realistic given constraints. | Clinician / Domain Expert |

---

## How TreeSHAP and DiCE Complement Each Other

| Aspect | TreeSHAP (Attributive) | DiCE (Counterfactual) |
|---------|------------------------|------------------------|
| **Explains** | *Why* a prediction was made | *What needs to change* for a different outcome |
| **Trust Type** | Internal reliability | Human actionability |
| **Metric Focus** | Fidelity, Completeness, Stability | Actionability, Diversity, Plausibility |
| **Dimension** | Technical | Social |
| **Interaction** | TreeSHAP provides a baseline for model coherence; DiCE tests if explanations translate into meaningful actions. |

---

## Next Steps

1. Compare SHAP and DiCE results to identify **alignment or tension** between technical reliability and human usability.  
2. Extend the analysis to classification and visual tasks (Pilots B and C).  
3. Integrate metrics into a **multidimensional trust dashboard** for comparative evaluation across techniques.

---

*This case empirically validates that both TreeSHAP and DiCE can operationalize measurable trust metrics — combining technical reliability with human-centered actionability — in a transparent and reproducible experimental setup.*
